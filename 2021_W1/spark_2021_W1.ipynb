{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2021: Week 1\n",
    "January 06, 2021\n",
    "\n",
    "Hello World...\n",
    "\n",
    "Welcome to 2021 Preppin' Data. \n",
    "\n",
    "Since Jonathan (@JonathanAllenby) and I chatted by a coffee machine in Feb 2019, we along with Jenny Martin (@JennyMartinDS14) and Tom Prowse (@TomProwse1) have produced 99 challenges to date, completed by 305 different people, who have solved the challenges 2,424 times (that we know of!). \n",
    "\n",
    "The overall aim of Preppin' Data is to give people a place to learn the power of being able to prepare your own data sets (rather than relying on others) and the capabilities of Tableau Prep. If you want to know more, check out our first post.\n",
    "\n",
    "For those of you who are new to Preppin' Data, here are some basics:\n",
    "\n",
    "We release a new challenge each Wednesday\n",
    "We post a written and video solution the following Tuesday\n",
    "We build our challenges in Tableau Prep but welcome solutions from any other tool too. \n",
    "One change this year is Jonathan is taking a step back from the challenges but will come up with a tricky problem every now and then I am sure. This means for our regulars you don't need to tag him on Twitter anymore. \n",
    "New Year, New Challenges\n",
    "Each January people all over the world might make resolutions to start a regular challenge to practice their skills so we wanted to facilitate that as best we could for a newer crowd to Prep Builder. \n",
    "\n",
    "The aim of January's challenges are to give you an introduction to Tableau Prep Builder. By going through these challenges you will be able to build fundamental skills with the tool, as well as data preparation skills applicable to all data work. \n",
    "\n",
    "Learning Resources\n",
    "\n",
    "- Tableau Videos - how I learnt Desktop originally and works well for getting the basics on Prep Builder too. There are 12 short videos but you won't need to watch them all at once. You will need to sign up for a free Tableau account to watch the videos. \n",
    "\n",
    "- Jenny & Tom's excellent free online training series - currently 31 short videos going into more precision on the features of Prep. Did I mention free?\n",
    "\n",
    "- Tableau Prep Up & Running - if you prefer your learning through words on a page, I wrote this book for beginner and intermediate users for Tableau Prep. \n",
    "\n",
    "Once you complete the challenge, you can post on our tracker, share an image (annotated if you want bonus points) on Twitter or the Tableau Forums. \n",
    "\n",
    "# Week 1's Challenge\n",
    "\n",
    "Challenge by Carl Allchin\n",
    "\n",
    "This week we are going to be focusing on cleaning data ready to answer some questions from our stakeholders. In the requirements I will be adding some links to some useful resources if you get stuck on a particular requirement. \n",
    "\n",
    "# Input\n",
    "The input is a csv file (Text File input type on prep Builder). The file looks like this (just with a lot more rows).\n",
    "\n",
    "<img src='https://1.bp.blogspot.com/-m11G7WH-ftY/X_NSuNX2e6I/AAAAAAAACFc/Zs4l5XIgQv8p3OflKifXhN8Y924iGJpHgCLcBGAsYHQ/w640-h240/Screenshot%2B2021-01-04%2Bat%2B17.38.47.png'>\n",
    "\n",
    "There are 1,000 orders.\n",
    "\n",
    "# Requirements\n",
    "Here's what we need you to do:\n",
    "\n",
    "* Connect and load the csv file (help)\n",
    "* Split the 'Store-Bike' field into 'Store' and 'Bike' (help)\n",
    "* Clean up the 'Bike' field to leave just three values in the 'Bike' field (Mountain, Gravel, Road) (help)\n",
    "* Create two different cuts of the date field: 'quarter' and 'day of month' (help)\n",
    "* Remove the first 10 orders as they are test values (help)\n",
    "* Output the data as a csv (help)\n",
    "\n",
    "# Output\n",
    "\n",
    "<img src='https://1.bp.blogspot.com/-5dcGmoprvTg/X_NVYmeoxEI/AAAAAAAACFo/U38HGpwxTIwzdBPyOQspdLX9rRnLsfjGwCLcBGAsYHQ/w640-h258/Screenshot%2B2021-01-04%2Bat%2B17.50.16.png'>\n",
    "\n",
    "8 Data Fields\n",
    "* Quarter\n",
    "* Day of Month\n",
    "* Store\n",
    "* Bike\n",
    "* Order ID\n",
    "* Customer Age\n",
    "* Bike Value\n",
    "* Existing Customer?\n",
    "990 Rows (991 including Column Headers)\n",
    "\n",
    "# Bonus task\n",
    "\n",
    "We are conscious this shouldn't be too much of a stretch for our Preppin' regulars so we've set you a Desktop task too. \n",
    "\n",
    "Our stakeholder wants to know the average monthly bike value sold by each day in the month. Yeah, they even want the running total to see where they should be in terms of sales by that point. The stakeholder knows each quarter is significantly different so the running totals should be separated by quarter. \n",
    "\n",
    "Build this view using the output if you fancy taking on the extra task. \n",
    "\n",
    "<img src = 'https://1.bp.blogspot.com/-fA_a39JBRGg/X_NWG9PKLrI/AAAAAAAACF0/ZJRb6OhBSC0_KPlppEewDX3Sodj93OHIACLcBGAsYHQ/w400-h359/Screenshot%2B2021-01-04%2Bat%2B11.54.29.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Could not parse Master URL: 'http://localhost:4040'\r\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2992)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:573)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m findspark\u001b[39m.\u001b[39mfind()\n\u001b[0;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n\u001b[1;32m----> 5\u001b[0m sc \u001b[39m=\u001b[39m SparkContext(\u001b[39m\"\u001b[39;49m\u001b[39mhttp://localhost:4040\u001b[39;49m\u001b[39m\"\u001b[39;49m,\u001b[39m\"\u001b[39;49m\u001b[39mTest-app\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkSession\n\u001b[0;32m      7\u001b[0m spark\u001b[39m=\u001b[39mSparkSession\u001b[39m.\u001b[39mbuilder\u001b[39m.\u001b[39mappName(\u001b[39m'\u001b[39m\u001b[39mTest-app\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mgetOrCreate()\n",
      "File \u001b[1;32mc:\\users\\raguna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\\pyspark\\context.py:200\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    198\u001b[0m SparkContext\u001b[39m.\u001b[39m_ensure_initialized(\u001b[39mself\u001b[39m, gateway\u001b[39m=\u001b[39mgateway, conf\u001b[39m=\u001b[39mconf)\n\u001b[0;32m    199\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 200\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_do_init(\n\u001b[0;32m    201\u001b[0m         master,\n\u001b[0;32m    202\u001b[0m         appName,\n\u001b[0;32m    203\u001b[0m         sparkHome,\n\u001b[0;32m    204\u001b[0m         pyFiles,\n\u001b[0;32m    205\u001b[0m         environment,\n\u001b[0;32m    206\u001b[0m         batchSize,\n\u001b[0;32m    207\u001b[0m         serializer,\n\u001b[0;32m    208\u001b[0m         conf,\n\u001b[0;32m    209\u001b[0m         jsc,\n\u001b[0;32m    210\u001b[0m         profiler_cls,\n\u001b[0;32m    211\u001b[0m         udf_profiler_cls,\n\u001b[0;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    213\u001b[0m     )\n\u001b[0;32m    214\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:\n\u001b[0;32m    215\u001b[0m     \u001b[39m# If an error occurs, clean up in order to allow future SparkContext creation:\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop()\n",
      "File \u001b[1;32mc:\\users\\raguna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\\pyspark\\context.py:287\u001b[0m, in \u001b[0;36mSparkContext._do_init\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menvironment[\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39menviron\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mPYTHONHASHSEED\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m0\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    286\u001b[0m \u001b[39m# Create the Java SparkContext through Py4J\u001b[39;00m\n\u001b[1;32m--> 287\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc \u001b[39m=\u001b[39m jsc \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_initialize_context(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conf\u001b[39m.\u001b[39;49m_jconf)\n\u001b[0;32m    288\u001b[0m \u001b[39m# Reset the SparkConf to the one actually used by the SparkContext in JVM.\u001b[39;00m\n\u001b[0;32m    289\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_conf \u001b[39m=\u001b[39m SparkConf(_jconf\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jsc\u001b[39m.\u001b[39msc()\u001b[39m.\u001b[39mconf())\n",
      "File \u001b[1;32mc:\\users\\raguna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\\pyspark\\context.py:417\u001b[0m, in \u001b[0;36mSparkContext._initialize_context\u001b[1;34m(self, jconf)\u001b[0m\n\u001b[0;32m    413\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    414\u001b[0m \u001b[39mInitialize SparkContext in function to allow subclass specific initialization\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    416\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jvm \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 417\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jvm\u001b[39m.\u001b[39;49mJavaSparkContext(jconf)\n",
      "File \u001b[1;32mc:\\users\\raguna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\\py4j\\java_gateway.py:1587\u001b[0m, in \u001b[0;36mJavaClass.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1581\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCONSTRUCTOR_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1582\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_command_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1583\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1584\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1586\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_gateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1587\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1588\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gateway_client, \u001b[39mNone\u001b[39;49;00m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_fqn)\n\u001b[0;32m   1590\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1591\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32mc:\\users\\raguna\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.10_qbz5n2kfra8p0\\localcache\\local-packages\\python310\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[39m=\u001b[39m OUTPUT_CONVERTER[\u001b[39mtype\u001b[39m](answer[\u001b[39m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[39mif\u001b[39;00m answer[\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[39mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mAn error occurred while calling \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m{1}\u001b[39;00m\u001b[39m{2}\u001b[39;00m\u001b[39m. Trace:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{3}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[39mformat\u001b[39m(target_id, \u001b[39m\"\u001b[39m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.\n: org.apache.spark.SparkException: Could not parse Master URL: 'http://localhost:4040'\r\n\tat org.apache.spark.SparkContext$.org$apache$spark$SparkContext$$createTaskScheduler(SparkContext.scala:2992)\r\n\tat org.apache.spark.SparkContext.<init>(SparkContext.scala:573)\r\n\tat org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\r\n\tat java.base/java.lang.reflect.Constructor.newInstance(Constructor.java:490)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:238)\r\n\tat py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)\r\n\tat py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('Test-app').getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyspark\n",
    "import pyspark.pandas as ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = 'PD 2021 Wk 1 Input.xlsx'\n",
    "df = ps.read_excel(io=input, sheet_name='Bike Sales')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input = 'PD 2021 Wk 1 Input.xlsx'\n",
    "# df = pd.read_excel(input, sheet_name=\"Bike Sales\")\n",
    "# df.drop(columns='Existing Customer?',inplace=True)\n",
    "# df.columns = df.columns.str.replace(' ','_')\n",
    "# df.columns = df.columns.str.replace('-','')\n",
    "# df = df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.read.csv('output.csv')\n",
    "spark_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark_df = spark.createDataFrame(df.astype(str))\n",
    "spark_df.printSchema()\n",
    "spark_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c3cedd81013b3798254e530f0d04430997cc2a4c554aece6737b52869c4f2a57"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
